{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page  0 , count:  15\n",
      "page  1 , count:  20\n",
      "page  2 , count:  20\n",
      "page  3 , count:  20\n",
      "page  4 , count:  20\n",
      "page  5 , count:  10\n",
      "page  6 , count:  20\n",
      "page  7 , count:  20\n",
      "page  8 , count:  20\n",
      "page  9 , count:  20\n",
      "page  10 , count:  20\n",
      "page  11 , count:  20\n",
      "page  12 , count:  20\n",
      "page  13 , count:  20\n",
      "page  14 , count:  20\n",
      "page  15 , count:  20\n",
      "page  16 , count:  20\n",
      "page  17 , count:  20\n",
      "page  18 , count:  20\n",
      "page  19 , count:  20\n",
      "page  20 , count:  20\n",
      "page  21 , count:  20\n",
      "page  22 , count:  20\n",
      "page  23 , count:  20\n",
      "page  24 , count:  20\n",
      "page  25 , count:  20\n",
      "page  26 , count:  20\n",
      "page  27 , count:  20\n",
      "page  28 , count:  20\n",
      "page  29 , count:  20\n",
      "page  30 , count:  20\n",
      "page  31 , count:  20\n",
      "page  32 , count:  20\n",
      "page  33 , count:  7\n",
      "page  34 , count:  0\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "\n",
    "def wait ():    \n",
    "    time.sleep(0.5+(random.randrange(0, 10)*0.05))\n",
    "\n",
    "def \n",
    "    \n",
    "url = \"https://nn.hh.ru/search/vacancy\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"clusters\": False,\n",
    "    \"ored_clusters\": False,\n",
    "    \"enable_snippets\": False,\n",
    "    \"salary\": None,\n",
    "    \"st\": \"searchVacancy\",\n",
    "    \"text\": \"golang\",\n",
    "    \"page\": 0\n",
    "}\n",
    "\n",
    "vacancys_data = []\n",
    "vacancys_on_page = True \n",
    "\n",
    "while vacancys_on_page:\n",
    "    \n",
    "    response = requests.get(url, headers = headers, params = params)\n",
    "    response_parsed = soup(response.text, 'html.parser')\n",
    "    \n",
    "    # ищем блоки с вакансиями и выдергиваем url\n",
    "    vacancys_on_page = response_parsed.select(\"div[class=vacancy-serp-item]\")  \n",
    "    vacancys_on_page = (vacancys_on_page if len(vacancys_on_page or [])>0 else False)\n",
    "    \n",
    "    # если нашли вакансии - составляем под них структуры с данными\n",
    "    if vacancys_on_page:\n",
    "        for vacancy_block in vacancys_on_page:\n",
    "            link_container = vacancy_block.select(\"a[class='bloko-link']\")\n",
    "            vacancys_data.append({\"url\": link_container[0]['href']})\n",
    "    \n",
    "    print(\"page \", params['page'], \", count: \", len(vacancys_on_page or []) )    \n",
    "    params['page'] += 1\n",
    "    \n",
    "    # симулируем\n",
    "    wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'verified_company' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d253e1e8e60f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# проверенный работодатель\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mverified_company\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# симулируем\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'verified_company' is not defined"
     ]
    }
   ],
   "source": [
    "# разбираем страницу вакансии\n",
    "counter = 1\n",
    "for data in vacancys_data:\n",
    "    \n",
    "    response = requests.get(data['url'], headers = headers)\n",
    "    response_parsed = soup(response.text, 'html.parser')    \n",
    "   \n",
    "    # вытаскиваем заголовок\n",
    "    \n",
    "    title = response_parsed.select(\"h1[data-qa = 'vacancy-title']\")\n",
    "    title = title[0] or False     \n",
    "    \n",
    "    data[\"title\"] = str(title.get_text()) \n",
    "    #if title:\n",
    "    #    for span in title.select(\"span\"):\n",
    "    #        if \"title\" in data: \n",
    "    #            data[\"title\"] = str(data[\"title\"])+\" \"+str(span.get_text())\n",
    "    #        else:\n",
    "    #            data[\"title\"] = str(span.get_text()) \n",
    "                        \n",
    "    # зарплата бля (\n",
    "    \n",
    "    salary_span = response_parsed.select(\"span[data-qa = 'bloko-header-2'][class = 'bloko-header-2 bloko-header-2_lite']\")\n",
    "    salary_span = salary_span[0] or False\n",
    "    \n",
    "    #print(counter,\" of \",len(vacancys_data),\" : \", salary_span.get_text())\n",
    "    counter += 1\n",
    "    \n",
    "    data[\"salary\"] = str(salary_span.get_text())\n",
    "    \n",
    "    # название компании\n",
    "    \n",
    "    company_name_container = response_parsed.select(\"a[data-qa = 'vacancy-company-name'][class = 'vacancy-company-name'] > span[data-qa = 'bloko-header-2']\")\n",
    "    company_name_container = company_name_container[0] or False \n",
    "    \n",
    "    data[\"company_name\"] = company_name_container.get_text()\n",
    "    \n",
    "    # проверенный работодатель\n",
    "    \n",
    "    verified_company\n",
    "    \n",
    "    # симулируем\n",
    "    wait()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 1 commit.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/russianSlave/hh_ru_parser.git\n",
      "   fe86cee..fe941d0  main -> main\n"
     ]
    }
   ],
   "source": [
    "! git add *\n",
    "! git commit \n",
    "! git push \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
